{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objetivo\n",
    "\n",
    "* El objetivo de la prueba es idear una solución para identificar transacciones que evidencian un \n",
    "comportamiento de Mala Práctica Transaccional, empleando un producto de datos. Adicional, \n",
    "describir la solución y detallar cómo incorporar el producto de datos en un marco operativo.\n",
    "\n",
    "### Datos relevantes \n",
    "\n",
    "* Se entiende como una Mala Práctica Transaccional, un comportamiento donde se evidencia un \n",
    "uso de los canales mal intencionado; como Fraccionamiento transacional.\n",
    "* Fraccionamiento transacional: esta es una mala practica que consiste en fraccionar una transacion grande en varias pequeñas, estas transacciones se caracterizan por estar en una misma ventana de tiempo que suele ser 24 horas y tienen como origen o destino la misma cuenta o cliente. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MOdelo identificacion de Transacciones Fracionadas: \n",
    "*  Genaracion de Clientes identificados como practicantes de fracionamiento transacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##########################################################################\n",
    "\n",
    "* Intalacion de Bibliotecas\n",
    "* Lectura de bibliotecas\n",
    "* Descarga de datos y/o creacion de Pipeline\n",
    "* Lectura de datos\n",
    "* identificacion de usuarios que tienen mas de 2 transaciones en un solo dia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intalacion librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "## Funcion para revisar si la libreria ya esta instalada, si no esta la instala.\n",
    "def ensure_library_installed(library_name):\n",
    "    try:\n",
    "        # Intenta importar la librería\n",
    "        importlib.import_module(library_name)\n",
    "        print(f\"La librería '{library_name}' ya está instalada.\")\n",
    "    except ImportError:\n",
    "        print(f\"La librería '{library_name}' no está instalada. Instalando ahora...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", library_name])\n",
    "        print(f\"Librería '{library_name}' instalada correctamente.\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "librerias =[\"gdown\",\"matplotlib\",\"pandas\",\"pyspark\",\"seaborn\",\"scikit-learn\",\"virtualenv\"]\n",
    "for i in librerias:\n",
    "    ensure_library_installed(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lectura de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import gdown\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from pyspark.sql.functions import to_timestamp, date_format\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, isnan, when, count, last, unix_timestamp, lag, to_date, round, sum as _sum\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, TimestampType, DecimalType\n",
    ")\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iniciar secion en spark y mejorar el rendimiento del cuaderno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Example App\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set the configuration\n",
    "spark.conf.set(\"spark.sql.repl.eagerEval.truncate\", 0)  # Use 0 to disable truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iniciar sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Descargar y cargar archivo\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Test\")\n",
    "    .config(\"spark.executor.memory\", \"16g\") \\\n",
    "    .config(\"spark.driver.memory\", \"16g\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Iniciar sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Descargar y cargar archivo\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# URL del archivo\n",
    "url = \"https://nequi-data.s3.us-east-1.amazonaws.com/sandbox_co/mscarmon/prueba_seleccion_ds/sample_data_0007_part_00.parquet\"\n",
    "\n",
    "# Ruta y nombre donde se guardará el archivo descargado\n",
    "output_path = \"C:/Users/jhonf/Descargas/Prueba_tecnica_Nequi/Data/sample_data_0007_part_00.parquet\"\n",
    "\n",
    "# 1. Verificar si el archivo existe y eliminarlo si es necesario\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "    print(f\"Archivo existente eliminado: {output_path}\")\n",
    "\n",
    "# 2. Descargar el archivo desde la URL\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:  # porque si el codigo esta en estado 200 es porque el servidor respondio de manera exitosa\n",
    "    with open(output_path, \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            file.write(chunk)\n",
    "    print(f\"Archivo descargado exitosamente como: {output_path}\")\n",
    "\n",
    "  # Leemos el archivo descargado\n",
    "    df7 = spark.read.parquet(output_path)\n",
    "    \n",
    "    # Mostrar algunas filas como verificación\n",
    "    df7.show(truncate=False)\n",
    "else:\n",
    "    print(f\"Error al descargar el archivo: {response.status_code}\")\n",
    "\n",
    "print(df7.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Iniciar sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Descargar y cargar archivo\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# URL del archivo\n",
    "url = \"https://nequi-data.s3.us-east-1.amazonaws.com/sandbox_co/mscarmon/prueba_seleccion_ds/sample_data_0006_part_00.parquet\"\n",
    "\n",
    "# Ruta y nombre donde se guardará el archivo descargado\n",
    "output_path = \"C:/Users/jhonf/Descargas/Prueba_tecnica_Nequi/Data/sample_data_0006_part_00_modelo.parquet\"\n",
    "\n",
    "# 1. Verificar si el archivo existe y eliminarlo si es necesario\n",
    "if os.path.exists(output_path):\n",
    "    os.remove(output_path)\n",
    "    print(f\"Archivo existente eliminado: {output_path}\")\n",
    "\n",
    "# 2. Descargar el archivo desde la URL\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:  # porque si el codigo esta en estado 200 es porque el servidor respondio de manera exitosa\n",
    "    with open(output_path, \"wb\") as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            file.write(chunk)\n",
    "    print(f\"Archivo descargado exitosamente como: {output_path}\")\n",
    "\n",
    "  # Leemos el archivo descargado\n",
    "    df6 = spark.read.parquet(output_path)\n",
    "    \n",
    "    # Mostrar algunas filas como verificación\n",
    "    df6.show(truncate=False)\n",
    "else:\n",
    "    print(f\"Error al descargar el archivo: {response.status_code}\")\n",
    "\n",
    "print(df6.count())\n",
    "print(df6.select(\"user_id\").distinct().count())\n",
    "print(df6.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para corregir el nombre de la columna\n",
    "def corregir_columna(columna):\n",
    "    # Eliminar espacios al principio y al final\n",
    "    columna = columna.strip()\n",
    "    columna = columna.replace(\"%\", \"pct\")\n",
    "    columna = columna.replace(\" \", \"_\").replace(\".\", \"_\")\n",
    "    columna = columna.replace(\"'\", \"\").replace('\"', '')\n",
    "    columna = columna.replace('//', '')\n",
    "    columna = columna.replace(\"#\", \"\")\n",
    "    return columna\n",
    "\n",
    "for columna in df6.columns:\n",
    "    # Renombrar cada columna usando la función corregir_columna\n",
    "    df6 = df6.withColumnRenamed(columna, corregir_columna(columna))\n",
    "    df7 = df7.withColumnRenamed(columna, corregir_columna(columna))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unimos toda la informacion\n",
    "df6=df6.union(df7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df6.groupBy(\"transaction_type\").count().show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar duplicados basándose en la columna \"_id\"\n",
    "dfu = df6.dropDuplicates([\"_id\"]) # Se deben quitar para ver cuantas transaciones eran iguales y encontrar posibles fraudes\n",
    "dfu = dfu.dropDuplicates([\"user_id\", \"transaction_date\"])\n",
    "print(dfu.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Liberar espacio en memoria\n",
    "del df6\n",
    "del duplicates\n",
    "del df6_users\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que la columna transaction_date esté en formato timestamp\n",
    "dfu = dfu.withColumn(\"transaction_time\", to_timestamp(col(\"transaction_date\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "dfu = dfu.orderBy(\"user_id\", \"transaction_time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculo de las ventanas de tiempo y diferencia en horas entre transaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1) Definir la columna 'day' ===\n",
    "dfu = dfu.withColumn(\"day\", to_date(col(\"transaction_time\")))\n",
    "\n",
    "# === 2) Detectar cambio de día con lag(day) ===\n",
    "# Ventana principal: particionamos por user_id y ordenamos por transaction_time\n",
    "w_user_order = Window.partitionBy(\"user_id\").orderBy(\"transaction_time\")\n",
    "\n",
    "dfu = dfu.withColumn(\n",
    "    \"prev_day\",\n",
    "    lag(\"day\").over(w_user_order)\n",
    ")\n",
    "\n",
    "# Creamos una bandera day_change que es 1 cuando cambia el día, 0 si es el mismo día\n",
    "dfu = dfu.withColumn(\n",
    "    \"day_change_flag\",\n",
    "    when(col(\"prev_day\").isNull(), 0)  # primera transacción => no cambia día\n",
    "    .when(col(\"day\") != col(\"prev_day\"), 1)\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# === 3) day_group: identificador acumulado de cada día, por usuario ===\n",
    "# Sumar en forma acumulada la bandera day_change_flag\n",
    "dfu = dfu.withColumn(\n",
    "    \"day_group\",\n",
    "    _sum(\"day_change_flag\").over(w_user_order)\n",
    ")\n",
    "# Así, cada vez que day_change_flag = 1, se incrementa day_group en 1\n",
    "\n",
    "# --- Limpieza opcional ---\n",
    "dfu = dfu.drop(\"prev_day\", \"day_change_flag\")\n",
    "\n",
    "# === 4) Calcular diff_hours dentro de cada día_group ===\n",
    "#  (1) Definir ventana que particiona por user_id y day_group, ordena por fecha/hora\n",
    "w_user_day = Window.partitionBy(\"user_id\", \"day_group\").orderBy(\"transaction_time\")\n",
    "\n",
    "# (2) Calcular la transacción anterior dentro del mismo day_group\n",
    "dfu = dfu.withColumn(\n",
    "    \"prev_tr_time_tmp\",\n",
    "    lag(\"transaction_time\").over(w_user_day)\n",
    ")\n",
    "\n",
    "# (3) Si es la primera transacción del day_group, prev_tr_time_tmp estará en null\n",
    "#     Asignamos la transaction_time actual para que diff_hours = 0 en esa fila\n",
    "dfu = dfu.withColumn(\n",
    "    \"prev_tr_time\",\n",
    "    when(col(\"prev_tr_time_tmp\").isNull(), col(\"transaction_time\"))\n",
    "    .otherwise(col(\"prev_tr_time_tmp\"))\n",
    ")\n",
    "dfu = dfu.drop(\"prev_tr_time_tmp\")\n",
    "\n",
    "# (4) Calcular diff_hours\n",
    "dfu = dfu.withColumn(\n",
    "    \"diff_hours\",\n",
    "    (unix_timestamp(\"transaction_time\") - unix_timestamp(\"prev_tr_time\")) / 3600\n",
    ")\n",
    "\n",
    "dfu = dfu.withColumn(\n",
    "    \"diff_minutes\",\n",
    "    (unix_timestamp(\"transaction_time\") - unix_timestamp(\"prev_tr_time\")) / 60\n",
    ")\n",
    "\n",
    "# === 5) new_window_flag: si diff_hours > 24, se abre una nueva ventana ===\n",
    "dfu = dfu.withColumn(\n",
    "    \"new_window_flag\",\n",
    "    when(col(\"diff_hours\") > 24, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# === 6) Calcular windows_time dentro de cada day_group ===\n",
    "#     - sumamos en forma acumulada new_window_flag y le sumamos 1 para iniciar en 1\n",
    "dfu = dfu.withColumn(\n",
    "    \"windows_time\",\n",
    "    _sum(\"new_window_flag\").over(w_user_day) + 1\n",
    ")\n",
    "\n",
    "# (Opcional) limpiar columnas\n",
    "dfu = dfu.drop(\"new_window_flag\",\"windows_time\")\n",
    "dfu = dfu.withColumnRenamed(\"day_group\", \"windows_time\")\n",
    "# Ajustar la columna \"day_group\" para que inicie en 1\n",
    "dfu = dfu.withColumn(\"windows_time\", col(\"windows_time\") + 1)\n",
    "# dfu.show(truncate=False)\n",
    "dfu = dfu.withColumn(\"diff_hours\", round(col(\"diff_hours\"), 2))\n",
    "dfu = dfu.withColumn(\"diff_minutes\", round(col(\"diff_minutes\"), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificacion de usuarios que tienen  mas de dos transaciones en un solo dia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agrupar por 'user_id' y 'windows_time', contar los 'windows_time' y sumar 'transaction_amount'\n",
    "df_count = dfu.groupBy(\"user_id\", \"windows_time\") \\\n",
    "    .agg(\n",
    "        F.count(\"windows_time\").alias(\"windows_time_count\"),\n",
    "        F.sum(\"transaction_amount\").alias(\"total_transaction_amount\"),\n",
    "        F.avg(\"diff_hours\").alias(\"avg_diff_hours\")  # Promedio de diff_hours\n",
    "    )\n",
    "# Filtrar aquellos 'user_id' donde el conteo de 'windows_time' sea mayor a 2\n",
    "df_filtered = df_count.filter(F.col(\"windows_time_count\") > 2)\n",
    "# Ordenar por total_transaction_amount de mayor a menor\n",
    "df_filtered = df_filtered.orderBy(F.col(\"total_transaction_amount\").desc())\n",
    "# Mostrar el resultado\n",
    "print('# Usuarios unicos que estan haciendo Fraccionamiento transaccional',df_filtered.select(\"user_id\").distinct().count())\n",
    "df_filtered.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas relevantes y convertirlas a pandas\n",
    "columns_to_plot = ['avg_diff_hours','avg_diff_minutes', 'windows_time_count', 'total_transaction_amount']\n",
    "df_pandas = df_filtered.select(columns_to_plot).toPandas()\n",
    "\n",
    "# Configurar el tamaño del gráfico\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Crear el boxplot para todas las columnas seleccionadas\n",
    "sns.boxplot(data=df_pandas, orient=\"h\", palette=\"Set2\")\n",
    "\n",
    "# Personalización\n",
    "plt.title('Boxplot de columnas para detección de outliers', fontsize=14)\n",
    "plt.xlabel('Valores', fontsize=12)\n",
    "plt.yticks(range(len(columns_to_plot)), columns_to_plot, fontsize=10)\n",
    "plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar solo las columnas necesarias y convertirlas a pandas\n",
    "df_pandas2 = df_filtered.select('windows_time', 'total_transaction_amount').toPandas()\n",
    "\n",
    "# Crear el scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df_pandas2['windows_time'], df_pandas2['total_transaction_amount'], alpha=0.7, color='skyblue', edgecolor='black')\n",
    "\n",
    "# Personalización de la gráfica\n",
    "plt.title('Scatter Plot: windows_time vs total_transaction_amount', fontsize=14)\n",
    "plt.xlabel('Total Amount Sum', fontsize=12)\n",
    "plt.ylabel('Transaction Count', fontsize=12)\n",
    "plt.grid(alpha=0.5)\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de salida donde deseas guardar el archivo Parquet\n",
    "save_path = \"C:/Users/jhonf/Descargas/Prueba_tecnica_Nequi/Data/Clientes_con_mas_de_tres_tranferencias.parquet\"\n",
    "\n",
    "# Guardar el DataFrame como archivo Parquet\n",
    "df_filtered.write.mode(\"overwrite\").parquet(save_path)\n",
    "\n",
    "print(f\"Archivo guardado correctamente en {save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
